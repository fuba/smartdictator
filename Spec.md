Flutter macOS 日本語音声認識 PoC アプリケーション 仕様書
この仕様書では、Flutter を用いて開発する macOS 専用 PoC (Proof of Concept) アプリケーションの設計について説明します。本アプリの目的は、日本語の音声入力をローカル環境でテキスト化し（音声文字起こし）、そのテキストから発話中の言い直しや脱線（冗長表現や言い直した部分）を取り除いて再構成することです。テキスト整形にはローカルで動作する Ollama 上の Gemma 3 (4Bモデル) を使用し、必要に応じてその結果を英語に翻訳します。ネットワークに依存せず、すべての処理をローカルで完結させることが特徴です。
使用技術とFlutter依存パッケージ
Flutter (macOS) – クロスプラットフォームUIフレームワーク。今回はmacOSターゲットに限定して使用します。
whisper_flutter_new – オフライン音声認識（Speech-to-Text）用ライブラリ。WhisperモデルのC++実装（whisper.cpp）をFlutterから利用でき、日本語を含む多言語の音声をテキスト化可能です​
pub.dev
。
サポートモデル: Tiny, Base, Small, Medium, Large (v1/v2)​
pub.dev
。
推奨モデル: 日本語精度とパフォーマンスのバランスから Base または Small モデルを使用（実機テスト済み）。必要に応じて初回起動時にモデルを自動ダウンロード可能です。
Ollama + Gemma 3 (4B) – オープンソースのLLMサーバー Ollama 上で動作する Google提供の軽量モデル Gemma 3 (4B) を使用します​
ollama.com
。Gemma 3は約4億パラメータの多言語対応モデルで、128Kトークンの長大な文脈や140以上の言語を扱える高性能モデルです​
ollama.com
。ローカルPC上（MacのCPU上）で実行可能なため、本PoCのテキスト整形・翻訳処理に適しています。
Gemma 3 4B モデルはOllamaのモデルライブラリから取得可能で、モデルサイズ約3.3GB（量子化Q4_K_M）です​
ollama.com
​
ollama.com
。M1/M2 Macクラスのハードウェアで動作確認されています。
HTTPクライアント (Flutter側) – Flutterの標準httpパッケージ等を用いて、ローカルで稼働中の Ollama サーバーのREST APIにリクエストを送信します。OllamaはHTTPエンドポイントを提供しており、curl http://localhost:11434/api/generate のような形式でモデルへのプロンプト送信・テキスト生成が可能です​
github.com
。
オーディオ録音 – マイクから音声を録音するためにFlutter用のマイク録音プラグインを使用します。例えば record パッケージ（全プラットフォーム対応のマイク録音ライブラリ）を利用し、WAV形式で音声を一時ファイルに保存します。record は iOS/macOSではAVFoundation、WindowsではMediaFoundation等ネイティブの音声APIを用いて実装されており、追加の外部依存なく高品質な録音が可能です​
pub.dev
​
pub.dev
。
モジュール構成とデータフロー
本アプリケーションは機能ごとにモジュールを分け、Flutterのレイヤ構造（プレゼンテーションとロジックの分離）を意識した設計とします。主要なモジュールとその役割、および各モジュール間のデータフローを以下に示します。
UIモジュール（プレゼンテーション層）: Flutterのウィジェットで構成されるユーザーインタフェース部分です。録音ボタンやテキスト表示フィールド等のGUIコンポーネントを提供します。ユーザー操作のイベント（ボタン押下等）を検知し、ビジネスロジック層へ通知します。ロジック層から受け取ったテキスト結果を画面に表示する役割も担います。
音声認識モジュール（STTサービス層）: whisper_flutter_new パッケージを利用した音声->テキスト変換処理を担当します。UIから録音終了の指示を受け取り、マイクで録音した音声ファイルをWhisperモデルに入力して日本語のテキストに変換します。変換結果のテキストはテキスト整形モジュールへ受け渡されます。このモジュールは非同期処理で実行し、UIスレッドをブロックしないように配慮します（例: Future により処理完了をUIに通知）。
テキスト整形・翻訳モジュール（LLMサービス層）: Ollama上のGemma 3モデルへのプロンプト送信と応答処理を担当します。音声認識モジュールから渡された日本語テキスト（話し言葉の逐語録）を入力とし、不要な言い直しや脱線を除去した洗練テキストを生成するようGemmaに指示します。生成された整形テキストはUIに表示されます。また、ユーザーが翻訳を要求した場合、本モジュールが整形済み日本語テキストを再度Gemmaに渡し、英訳テキストを生成してUIへ返します。Gemmaへの問い合わせはHTTP経由で行い、結果取得も非同期で処理します。
データフローのシーケンス（1回の発話処理の流れ）:
ユーザーが「録音ボタン」を押し続けて発話します。ボタン押下によりUIモジュールは音声認識モジュールに録音開始を指示します。録音モジュールは record プラグイン等でマイク入力のキャプチャを開始します。
ユーザーがボタンを離す（録音停止）と、UIは録音停止イベントを音声認識モジュールに通知します。録音モジュールは音声データを一時的なWAVファイルに保存し、ファイルパスを取得します。
音声認識モジュールは Whisperモデルをロードし（初回のみ数秒程度モデル読み込み時間が発生）、録音されたWAV音声ファイルを入力として 日本語音声認識を実行します。Whisperがオフラインで音声を逐次または一括で解析し、発話内容の**テキスト（逐語起こし）**を得ます。処理後、音声ファイルは不要であれば削除します。
得られた**音声認識結果テキスト（生の逐語録）**をUIに表示します。同時に、そのテキストを整形・翻訳モジュールに渡して次の処理をトリガーします。UI上にはこの時点で「音声認識結果」欄に話し言葉そのままのテキストが表示されます。
テキスト整形モジュールは受け取った逐語録テキストを**Gemma 3 (4B)**モデルに送信します。具体的には、後述するプロンプトをHTTP経由で http://localhost:11434/api/generate エンドポイントにPOSTし、モデルにテキストの整形を依頼します​
github.com
。
Ollamaサーバーはリクエストを受け取り、Gemmaモデルにプロンプトを適用して推論を実行します。モデルは指示に基づき、入力文をきれいに言い直した整形済みテキストを生成し、応答します。
Flutter側のLLMサービス層はこの応答を受信し、テキスト部分を抽出します（不要な制御文字やプロンプトエコーが含まれる場合は除去）。
UIモジュールは整形モジュールからの結果を取得し、「修正後テキスト」欄に整形済み日本語テキストを表示します。これにより、ユーザーは音声認識直後の文章（逐語結果）と、LLMによる修正後文章を比較閲覧できます。多くの場合、修正後テキストでは言い直しや文の途中の修正箇所が整理され、読みやすい一つの文または段落になっている想定です。
（任意）ユーザーが「翻訳ボタン」をクリックすると、UIモジュールはテキスト整形モジュールに英訳リクエストを送ります。これにより、現在表示中の整形済み日本語テキストをGemmaモデルに再度プロンプト送信し、英訳テキストを生成します（プロンプト内容は後述）。
Gemmaモデルから返答された英語訳テキストを翻訳結果欄に表示します。翻訳ボタンを押すたびに最新の整形日本語テキストに基づいて英訳を行います（必要が無い場合、ユーザーは翻訳ボタンを押さなければ日本語テキストのみが表示された状態で完結します）。
以上のフローにより、ユーザーは録音操作から最終的な日本語テキストおよび必要に応じた英語訳を得ることができます。全処理はデバイス内で完結し、逐次処理（録音→文字起こし→整形→翻訳）の各段階は非同期に行われます。処理中はUI上でローディングインジケータを表示するなどし、ユーザーが次の録音開始などの操作を誤って行わないよう制御します（例: 処理中は録音ボタンを無効化するなど）。
UI構成
UIはシンプルかつ直感的に操作できるレイアウトとします。主な画面要素とレイアウトは以下の通りです（1画面構成）。
録音ボタン: 画面上部に大きく配置します。マイクのアイコン付きの円形ボタンで、「押して話す」操作を表現します。ユーザーはこのボタンをマウス押下で発話開始し、離すと録音停止する想定です（長押し録音形式）。録音中はアイコンやボタン色を変化させ、録音状態を分かりやすくフィードバックします。
音声認識結果表示: 録音停止後、Whisperによる音声認識の結果テキストを表示するエリアです。ラベル「音声認識結果」または「逐語テキスト」と共に、下にマルチラインのテキストボックスまたは読み取り専用のTextウィジェットで内容を示します。自動スクロールできるようにし、長い文章でも閲覧可能にします。
修正後テキスト表示: 音声認識結果がLLMで整形された後のテキストを表示するエリアです。ラベル「修正後テキスト」または「整形結果」とし、同様にその下にマルチラインテキスト表示を配置します。これも読み取り専用で、ユーザーがコピーできるようにすると便利です。整形結果は逐語テキストと区別できるよう、背景色を少し変える等の視覚的工夫をします。
翻訳ボタン: 修正後テキストの下または隣に配置するボタンです。ラベルは「英訳」などとし、ユーザーが必要なときに押せば現在の修正後テキストを英語翻訳します。初期状態では非アクティブ（音声認識結果が無い段階では押せない）とし、修正後テキストが表示された段階で有効にします。ボタン押下時には内部でLLMへの翻訳プロンプトを送り、結果を取得するまでスピナー等で待機状態を表示します。
翻訳結果表示: 翻訳ボタン押下後、生成された英語テキストを表示するエリアです。ラベル「英訳結果」と共に、テキスト表示を配置します。ここには最終的な英文のみが表示されます。日本語テキストと同様にコピー可能なテキストウィジェットとします。翻訳結果欄は、翻訳が行われていない時点では空欄か非表示にしておき、ボタン押下時に初めて表示させるか、あるいは常に表示領域を確保して「（未翻訳）」と表示するなどのUI設計も考えられます。
各表示欄の配置は縦に順番に並ぶシンプルなレイアウトとします（上から録音ボタン、認識結果、日本語整形結果、翻訳ボタン、英訳結果の順）。スクロール可能な領域を必要に応じて設け、ウィンドウサイズが小さい場合でも内容を確認できるようにします。全体としてMacのウィンドウにコンパクトに収まるUIでありつつ、テキストは十分な横幅を取って改行されず表示できるようデザインします。
Gemma 3 (4B) に送信するプロンプト設計
音声認識結果のテキストから言い直しや脱線を除去し、一つの自然な文章または段落に再構成するためには、LLM（Gemma 3 4Bモデル）への指示内容（プロンプト）が重要です。本PoCでは、Gemmaに対して生成すべき最終結果のみを返すよう工夫したプロンプトを用います。ここでは日本語テキストの整形と、任意の英訳の2種類のプロンプト設計について説明します。
日本語テキスト整形プロンプト: 音声認識された日本語逐語テキストを入力とし、それをクリーンアップしてもらうための指示を与えます。ポイントは、言い直しや不要部分の削除と文章の再構成を明示し、さらに出力は修正後の文章のみになるよう求めることです。プロンプト例（システムメッセージあるいはユーザーメッセージとして送信）:
「以下の逐語録テキストを読みやすい日本語文に修正してください。話者が途中で言い直した部分や脱線した部分は取り除き、一つの自然な文章（または段落）に再構成してください。出力は修正後の文章のみを返してください。」
（続けて、整形対象となる逐語テキスト本文をプロンプトに含める）
このプロンプトにより、モデルは与えられた文章を校正・編集し、冗長な部分を省いた上で意味が通るよう再構成します。最後の一文で「出力は修正後の文章のみ」と指示しているため、モデルは編集結果のみを返し、説明や元の入力の繰り返しを行わないことが期待されます。Gemma 3は指示に従う汎用モデルであり、プロンプト内に不要な語句がなければ基本的に返答も必要最小限になります。万一モデルがプロンプトをそのまま繰り返したり、謝罪や説明を付加してくる場合は、システムレベルの指示や出力フォーマット指定を加えて調整します。 実装上は、上記プロンプトを JSON の "prompt" フィールドに含め、HTTP POST で http://localhost:11434/api/generate に送信します。リクエストボディは例えば以下のようになります​
github.com
:
json
コピーする
{
  "model": "gemma3:4b",
  "prompt": "以下の逐語録テキストを読みやすい日本語文に修正してください…（省略）…出力は修正後の文章のみを返してください。\n\n「{ここにWhisperから得た逐語テキスト}」"
}
このリクエストに対し、OllamaはGemmaモデルからの応答をJSONで返します。応答の中の生成テキスト（通常{"data": [{"output": "..."}]}形式で得られる想定）を取り出し、それが整形済みテキストとなります。
英訳プロンプト: 日本語の整形済みテキストを英語に翻訳する場合の指示です。こちらも翻訳結果のみが出力されるようにします。プロンプト例:
「以下の日本語文を英語に翻訳してください。翻訳後の英文のみを出力してください。」
（続けて翻訳対象の日本語テキストを記載）
この指示により、モデルは与えられた日本語を英訳し、その英文のみを返します。翻訳に際しては原文のニュアンスをできるだけ保ちつつ、自然な英語にするよう期待します。Gemma 3は多言語モデルのため日本語から英語への翻訳能力も持ち合わせていますが、専門の翻訳モデルほどではない可能性もあるため、必要に応じて翻訳結果の品質確認を行います。プロンプトに「英文のみを出力」とあるため、余計な解説や日本語は含まれない想定です。 実装では、翻訳ボタン押下時に上記プロンプトを用いて再度 api/generate エンドポイントにリクエストします（"model": "gemma3:4b"は同じ、"prompt"のみ翻訳指示に差し替え）。モデルの応答から得られた英文テキストを抽出し、UIに表示します。なお、連続して同一モデルにリクエストを送る場合、Ollama側で前回の会話コンテキストが保持されていると不要な応答をする可能性があるため、/api/generate（常に単発プロンプトで生成）を使うことでコンテキストをリセットしています。これにより逐次のリクエストがお互い独立し、常に新鮮な1回分の生成結果のみを得ることができます。
プロンプト設計の工夫として、出力に余計なものを含めないよう「～のみ出力してください」「～だけ返してください」といった指示を明示的に含めています。また、Gemma 3モデル自体が定義するシステムプロンプトやテンプレートがある場合（Ollamaのモデル設定内でstopトークンやテンプレート指定がされていることがあります​
ollama.com
）、それらと競合しないよう簡潔で明確な指示文を心がけます。PoC段階では上記プロンプトで期待通り動作するかテストを重ね、必要に応じて調整します。
起動時の前提条件
本アプリを正常に動作させるために、以下の準備・設定が起動前に満たされている必要があります。
Ollama サーバーの起動: ローカルLLM推論用にOllamaがインストール済みであることが前提です。macOS環境にOllamaを導入し（Homebrewや公式インストーラ使用）、ollama serve コマンド等でローカルサーバーを起動しておきます。デフォルトではポート11434で待ち受けます​
github.com
。本PoCではアプリからOllamaを直接起動する機能は実装しないため、ユーザーが事前にOllamaサーバーを起動している必要があります。サービス形式でバックグラウンド起動していても構いません。
Gemma 3 4B モデルの準備: Ollama上でGemma 3 (4B)モデルが使用可能であることを確認します。初回使用時には ollama pull gemma3:4b などのコマンドでモデルをダウンロードしローカルにインストールしておきます（約3.3GBのストレージ空き容量が必要）​
ollama.com
。モデル名は gemma3:4b で認識されるため、APIリクエスト時もこの名前を指定します。モデルが未準備の場合、OllamaはAPI呼び出し時にエラーを返すか自動で取得を試みますが、PoCデモ時のスムーズな動作のため事前準備を推奨します。
Whisperモデルの準備: whisper_flutter_new 利用時に必要なWhisper音声認識モデルファイル（例: baseモデル約140MB）をアプリ内で使用可能にしておきます。方法として、
アプリ初回起動時にプラグインのダウンロード機能を使い所定のモデルを自動取得する
あるいは開発時にモデルファイルを同梱しAssetとして読み込む（ただしLargeモデル等はサイズが大きいため、本PoCではBaseかSmall程度を使用）
といった手段があります。モデルは日本語対応のためできれば Base 以上を使用します​
pub.dev
。モデルファイルが配置されていない場合、音声認識処理呼び出し時にエラーとなりますので注意してください。
マイク使用の許可: アプリがマイク入力にアクセスできるよう、ユーザーからの許可が必要です。初回の録音開始時にシステムのマイク使用許可ダイアログが表示されますので、ユーザーに許可してもらう必要があります。これを機能させるために、事前にアプリのInfo.plistに適切なキーを設定します（例: NSMicrophoneUsageDescription に利用目的のメッセージを記載）。さらに、Macアプリがサンドボックスまたは Hardened Runtime 環境で動作する場合、マイクアクセスのためのエンタイトルメント（com.apple.security.device.microphone および com.apple.security.device.audio-input）を付与する必要があります​
docs.flutter.dev
。開発中のデバッグビルドでは自動的に許可ダイアログが出ることを確認していますが、リリースビルドで配布する際にはこれら権限設定が正しく行われていることを確認してください。
ネットワーク権限: 本アプリ自体はインターネット接続を必要としませんが、ローカルホスト上のOllamaサーバーと通信するため、ループバック通信の許可が必要です。通常のmacOSアプリではローカルホスト通信もApp Sandboxのcom.apple.security.network.client権限が無いとブロックされる可能性があります​
docs.flutter.dev
。そのため、サンドボックス有効の場合はネットワーククライアント権限を付与し、ローカルHTTP通信を許可する設定にします。デバッグモードでは一時的に緩和されていますが、製品ビルドでは明示的な設定が必要となります。
以上の前提条件を満たした環境でアプリを起動すれば、録音〜テキスト化〜整形までの一連の処理がエラー無く実行できる見込みです。仮にこれらの条件が整っていない場合（例えばOllamaが起動していない、モデル未ダウンロード、マイク権限拒否など）、アプリ起動時または各機能実行時にユーザーへ警告メッセージを表示し、適切に対処するよう促します。
制限事項とPoC実装上の注意点
本アプリケーションはPoC段階の実装であり、プロダクション利用には以下のような制限事項や注意点があります。開発時にはこれらを念頭に置き、必要に応じて改善や対策を検討します。
処理速度とパフォーマンス: オフライン音声認識およびローカルLLM推論には比較的大きな計算コストがかかります。Whisperモデルによる音声認識は音声長に比例して時間がかかり、Gemma 3 (4B)モデルによるテキスト生成もCPU上で数秒程度要する可能性があります。短い発話（数秒程度）を前提としたPoCであるため実用上問題ありませんが、長時間の発話には向いていません。また、初回のモデルロード時間（Whisperモデル・Gemmaモデルともに）も考慮が必要です。一度ロードしたモデルはメモリ上に保持されるため2回目以降は高速化しますが、メモリ使用量増大にも注意してください。
精度と品質: Whisperによる文字起こしの精度はモデルサイズや音質に依存します。小さいモデルでは日本語の誤認識や誤変換が増える可能性があります。Gemmaによる文章整形も、音声認識結果に誤りがあると必ずしも意図した修正にならない可能性があります。LLMは文脈から推測して修正を試みますが、元の認識が間違っている部分はそのまま残ったり、逆に意図しない内容に書き換えられてしまうリスクもあります。PoCではこれらを受け入れつつデモを行いますが、必要であればユーザーに「自動整形結果であり誤りを含む可能性」がある旨を伝える配慮も検討します。
モデルの制約: Gemma 3 (4B)モデルは軽量とはいえ約4Bパラメータを持つLLMであり、非常に長い入力や高度な質問応答には大型モデルほど対応できない場合があります。本用途（逐語録のリライトや翻訳）には十分な性能が期待されますが、口語表現のニュアンスや専門用語の翻訳など完璧でないケースもあります。またGemmaは多言語モデルですが主に英語をベースにチューニングされている可能性もあり、日本語の細かな語調変換に限界があるかもしれません。必要に応じ、将来的には日本語特化で校正・要約に優れたモデルへの差し替えも検討できます。
リアルタイム性: 本PoCは録音終了後にまとめて処理を行うバッチ的な流れです。リアルタイムで字幕起こしのように逐語表示したり逐次翻訳する機能は実装していません。逐次処理中はユーザーは待つ必要があり、処理完了までのステータスを適切に表示する必要があります。将来的にリアルタイム性を高めるには、Whisperをストリーミング処理しながら部分的に結果表示する、LLM整形も並列で行う、といった拡張が考えられますが、PoCの範囲外です。
ユーザーインタラクション: PoCでは非常にシンプルなUIで必要最低限の操作しかありません。例えば録音の一時停止や再開、複数文を続けて入力するといった高度な操作は考慮していません。また翻訳結果をクリップボードにコピーする機能などユーザビリティ向上の要素も最小限です。デモや評価の際には、このシンプルさを前提にユーザーに操作してもらいます。
エラーハンドリング: ネットワーク未接続での利用を想定していますが、内部的にOllamaサーバーとHTTP通信を行うため、サーバープロセスがダウンしている・応答が遅延する・モデルが読み込めない等のエラーに備える必要があります。PoC実装では、LLM API呼び出しがタイムアウトした場合やエラーを返した場合に、UI上にエラーメッセージを表示する程度の簡易的なハンドリングを入れます。Whisper部分でも音声ファイルが無い、モデルがロードできない等の際に例外処理を行い、アプリがクラッシュしないよう留意します。
セキュリティと権限: 前提条件でも述べた通り、マイクアクセスやローカルネットワーク通信には適切な権限設定が必要です。デバッグ時には動いていたものがリリースビルドで権限不足により動かない、といった事態が起こりえます。特にmacOSではApp SandboxやHardened Runtimeの設定に注意が必要です​
docs.flutter.dev
。PoCでは開発環境で実行することから大きな問題にはなりませんが、将来的に配布・展開する際はこれらセキュリティ設定を精査してください。また、本アプリはローカルで完結するためユーザーデータを外部送信することはありませんが、マイクから取得した音声データや変換テキストはアプリ内で一時的に保持されます。プライバシー保護のため、それらを永続保存しない（処理後は音声ファイルを削除する、アプリ終了時にテキストをクリアする等）対策も検討します。
プラットフォーム依存: 現状macOS専用として作成しているため、他プラットフォーム（WindowsやLinux、iOS/Android）への展開には追加対応が必要です。特にwhisper_flutter_newはAndroid/iOSにも対応していますが、デスクトップWindows/Linuxでは利用可否を確認する必要があります。またOllamaはmacOS向けに最適化されていますが、Windows/Linux版も存在します。ただしモデルや処理能力の差異、マイク周りの許可手順の違いなどがあるため、マルチプラットフォーム化する際は個別検証が必要です。本PoCの目的範囲ではmacOSでの動作検証のみを行います。
以上のように、本PoCアプリケーションはオフライン環境で日本語音声の文字起こしとテキストクリーンアップを実現するものです。使用する技術スタック（WhisperとGemma 3）は最新のオープンソースを活用しており、それぞれ単独でも高い機能を持っています。本仕様書に従い実装を進め、動作検証を通じてオフライン音声対話の可能性を探ります。今後必要に応じてモデルの変更や機能拡充を行うことで、より完成度の高いアプリケーションへ発展させることが期待できます。


